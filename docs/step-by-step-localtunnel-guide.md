# Step-by-Step Guide: Local AI with LocalTunnel (No Tokens Required)

## What We're Building
- **Gemma 2 AI** running on Google Colab (your private AI service)
- **LocalTunnel** creates public URL (no authentication required)
- **ProcessGPT** connects to your private AI for manufacturing analysis

## Why This Works
- **No Tokens**: LocalTunnel is completely free, no signup required
- **Privacy**: Your manufacturing data never leaves your control
- **Speed**: 2-3 seconds per query response on CPU
- **Reliability**: Works with Google Colab free tier

---

## Step 1: Open Google Colab

### What to do:
1. Go to https://colab.research.google.com
2. Sign in with your Google account
3. Click "New Notebook"
4. **Optional**: Runtime ‚Üí Change runtime type ‚Üí Select "CPU" (we don't need GPU)

### Why this step:
Google Colab provides free cloud computing where we'll run our AI model. We're choosing CPU because it's more reliable for long-running services and perfectly adequate for interactive manufacturing analysis.

---

## Step 2: Install Dependencies

### Code to paste:
```python
# Step 2: Install all required dependencies
print("üì¶ Installing dependencies...")
!pip install flask requests
!npm install -g localtunnel

print("‚úÖ Dependencies installed successfully!")
```

### What this does:
- **Flask**: Web server framework to create API endpoints
- **Requests**: HTTP library for making API calls
- **LocalTunnel**: Creates public URLs without authentication
- **npm install -g**: Installs LocalTunnel globally in the Colab environment

### Expected output:
You'll see package installation logs. Wait for "Dependencies installed successfully!"

---

## Step 3: Install and Start Ollama

### Code to paste:
```python
# Step 3: Install and start Ollama AI runtime
print("ü§ñ Installing Ollama...")
!curl -fsSL https://ollama.ai/install.sh | sh

import subprocess, threading, time

def run_ollama():
    """Start Ollama server in background"""
    subprocess.run(["/usr/local/bin/ollama", "serve"])

print("üöÄ Starting Ollama service...")
ollama_thread = threading.Thread(target=run_ollama, daemon=True)
ollama_thread.start()
time.sleep(15)  # Give Ollama time to fully start

print("‚úÖ Ollama is running!")
```

### What this does:
- **Ollama**: AI model runtime that can run various language models
- **Background thread**: Starts Ollama server without blocking our main program
- **15 second wait**: Ensures Ollama is fully loaded before proceeding

### Expected output:
Installation logs, then "Ollama is running!" after about 30 seconds.

---

## Step 4: Download Gemma 2 Model

### Code to paste:
```python
# Step 4: Download Gemma 2 model (this takes 5-10 minutes)
print("üì• Downloading Gemma 2 model...")
print("‚è∞ This will take 5-10 minutes - please wait...")
!ollama pull gemma2:9b

print("üß™ Testing model...")
!ollama run gemma2:9b "Analyze manufacturing process efficiency" --verbose

print("‚úÖ Gemma 2 model ready!")
```

### What this does:
- **Downloads 5GB model**: Gemma 2 with 9 billion parameters
- **One-time download**: Model stays cached for future use
- **Test query**: Verifies the model works correctly
- **9b version**: Optimized for CPU performance

### Expected output:
Download progress bars for ~10 minutes, then a test response about manufacturing analysis.

### Important:
This is the longest step. The model download is a one-time process - once complete, your AI will respond in 2-3 seconds.

---

## Step 5: Create API Service

### Code to paste:
```python
# Step 5: Create Flask API that ProcessGPT can talk to
from flask import Flask, request, jsonify
import requests

app = Flask(__name__)

@app.route('/api/generate', methods=['POST'])
def generate():
    """Forward ProcessGPT requests to local Gemma 2"""
    try:
        data = request.json
        response = requests.post('http://localhost:11434/api/generate', 
                               json=data, timeout=60)
        return jsonify(response.json())
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/health', methods=['GET'])
def health():
    """Check if AI service is working"""
    try:
        test = requests.get('http://localhost:11434/api/tags', timeout=5)
        if test.status_code == 200:
            return jsonify({"status": "healthy", "service": "Colab + Gemma 2"})
        return jsonify({"status": "unhealthy"}), 500
    except:
        return jsonify({"status": "unhealthy"}), 500

print("üåê API service created!")
```

### What this does:
- **API Bridge**: Translates between ProcessGPT and Gemma 2
- **Error handling**: Graceful failure if something goes wrong
- **Health check**: ProcessGPT can verify the service is working
- **Timeout protection**: Prevents hanging requests

### Expected output:
"API service created!" message.

---

## Step 6: Create Public Tunnel

### Code to paste:
```python
# Step 6: Create public tunnel with LocalTunnel
import subprocess, threading

def create_tunnel():
    """Create public URL using LocalTunnel (no authentication required)"""
    try:
        print("üîó Creating public tunnel...")
        # Start LocalTunnel - connects port 5000 to public internet
        result = subprocess.run(['lt', '--port', '5000', '--subdomain', 'processgpt-ai'], 
                              capture_output=True, text=True, timeout=15)
        
        # Alternative: random subdomain if custom one fails
        if not result.stdout:
            result = subprocess.run(['lt', '--port', '5000'], 
                                  capture_output=True, text=True, timeout=15)
        
        if result.stdout:
            url = result.stdout.strip()
            print(f"\nüéâ SUCCESS! Your AI endpoint: {url}")
            print(f"üìã Copy this URL: {url}")
            print(f"üîß Set in ProcessGPT: OLLAMA_HOST={url}")
            print(f"‚öôÔ∏è  Set in ProcessGPT: USE_LOCAL_AI=true")
            return url
        else:
            print("‚ùå Tunnel creation failed, trying alternative method...")
            return None
            
    except Exception as e:
        print(f"‚ö†Ô∏è  Tunnel error: {e}")
        print("üí° Your AI runs locally at http://localhost:5000")
        return None

# Start tunnel in background
tunnel_thread = threading.Thread(target=create_tunnel, daemon=True)
tunnel_thread.start()
time.sleep(5)  # Give tunnel time to establish

print("‚úÖ Tunnel setup complete!")
```

### What this does:
- **Public URL**: Makes your private Colab accessible from internet
- **No authentication**: LocalTunnel doesn't require tokens or signup
- **Fallback options**: Tries multiple methods to ensure success
- **Background process**: Doesn't block the main server

### Expected output:
A public URL like `https://processgpt-ai.loca.lt` or similar.

---

## Step 7: Start the AI Service

### Code to paste:
```python
# Step 7: Start the complete AI service
print("\n" + "="*60)
print("üöÄ STARTING PROCESSGPT LOCAL AI SERVICE")
print("="*60)
print("‚úÖ Gemma 2 model: Ready")
print("‚úÖ API endpoints: Created") 
print("‚úÖ Public tunnel: Active")
print("‚úÖ Service status: Starting...")
print("\nüìç Keep this notebook running to maintain the AI service")
print("üîÑ ProcessGPT will now use your private Gemma 2 instead of OpenAI")
print("="*60)

# Start the Flask server (this keeps running)
app.run(host='0.0.0.0', port=5000, debug=False)
```

### What this does:
- **Starts web server**: Makes your AI accessible via HTTP
- **Runs continuously**: Keeps service alive for ProcessGPT
- **Status messages**: Confirms everything is working
- **Debug off**: Reduces log noise for cleaner operation

### Expected output:
Server startup messages, then it runs continuously waiting for ProcessGPT requests.

---

## Step 8: Configure ProcessGPT

### What to do in your ProcessGPT:
1. Set environment variable: `USE_LOCAL_AI=true`
2. Set environment variable: `OLLAMA_HOST=https://your-tunnel-url.loca.lt`
3. Restart your ProcessGPT application
4. Test by asking: "Which hour has the highest concentration of failures?"

### Why this works:
ProcessGPT will now send all AI queries to your private Gemma 2 instead of OpenAI, while maintaining all 25 analysis functions and your manufacturing data privacy.

---

## Troubleshooting

### If tunnel fails:
- Try running the tunnel step again
- LocalTunnel sometimes takes a few attempts
- Your AI still works locally at localhost:5000

### If model is slow:
- 2-3 seconds per response is normal on CPU
- First query may take longer as model warms up
- Subsequent queries will be faster

### If connection fails:
- Check the tunnel URL is accessible in browser
- Verify ProcessGPT environment variables are set correctly
- Restart the Colab notebook if needed

---

## Success Indicators

‚úÖ **Colab shows**: "Your AI endpoint: https://something.loca.lt"
‚úÖ **ProcessGPT works**: AI Assistant responds to manufacturing questions  
‚úÖ **Data privacy**: No external API calls for your manufacturing data
‚úÖ **Function preservation**: All 25 analysis types still work
‚úÖ **Speed**: 2-3 second response times